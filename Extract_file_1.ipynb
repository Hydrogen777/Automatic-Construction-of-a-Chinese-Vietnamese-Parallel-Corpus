{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from xlsxwriter import Workbook\n",
    "import easyocr\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "file_name = \"output.xlsx\"\n",
    "reader = easyocr.Reader(['vi'], gpu=True)\n",
    "json_folder = \"./Json_result\"\n",
    "img_folder = \"./output_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_position(json_data):\n",
    "    extracted_data = []\n",
    "    \n",
    "    # Lặp qua các dòng văn bản trong 'text_lines'\n",
    "    for line in json_data['data']['text_lines']:\n",
    "        line_info = {\n",
    "            'text': line['text'],\n",
    "            'position': line['position']\n",
    "        }\n",
    "        \n",
    "        # Thêm line_info vào kết quả\n",
    "        extracted_data.append(line_info)\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "def print_text_only(extracted_data):\n",
    "    data = []\n",
    "    for item in extracted_data:\n",
    "        data.append(str(item['text']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Is_y0(s):\n",
    "    \"\"\"Điều kiện để xác định loại y0.\"\"\"\n",
    "    return re.match(r'^(a\\.|g\\.|a,|g,|\\d+\\.$|^\\d+\\s\\.$)$', s, re.IGNORECASE) or s in {\"a\", \"g\"}\n",
    "\n",
    "def Is_y1(s):\n",
    "    \"\"\"Điều kiện để xác định loại y1.\"\"\"\n",
    "    return re.match(r'^(b\\.)$', s, re.IGNORECASE) or s.startswith(\"b.\") or s.startswith(\"b :\") or s.startswith(\"b:\") or s.startswith(\"b .\") or s in {\"b \", \n",
    "\"b_\", \"b,\", \"b Phiên âm:\", \"b. Phiên âm:\", \"b.Phiên âm:\", \"b_Phiên âm:\", \"b Phiên âm\", \"b. Phiên âm\", \"b.Phiên âm\", \"b_Phiên âm\", \n",
    "\"b Phien am:\", \"b. Phien am:\", \"b.Phien am:\", \"b_Phien am:\", \"b Phien am\", \"b. Phien am\", \"b.Phien am\", \"b_Phien am\", \n",
    "\"Phiên âm:\", \"Phiên âm:\", \" Phiên âm:\", \"Phiên âm:\", \"Phiên âm\", \" Phiên âm\", \"Phiên âm\", \"Phiên âm\", \n",
    "\"Phien am:\", \" Phien am:\", \"Phien am:\", \"Phien am:\", \"Phien am\", \" Phien am\", \"Phien am\", \"Phien am\"}\n",
    "\n",
    "def Is_y2(s):\n",
    "    \"\"\"Điều kiện để xác định loại y1.\"\"\"\n",
    "    return re.match(r'^(c\\.)$', s, re.IGNORECASE) or s.startswith(\"c.\") or s.startswith(\"c \") or s.startswith(\"c :\") or s.startswith(\"c:\") or s.startswith(\"C.\") or s.startswith(\"C \") or s.startswith(\"C:\") or s.startswith(\"c:\") or s in {\"c \", \n",
    "\"c_\", \"c,\", \"c Dịch nghĩa:\", \"c. Dịch nghĩa:\", \"c.Dịch nghĩa:\", \"c_Dịch nghĩa:\", \"c Dịch nghĩa\", \"c. Dịch nghĩa\", \"c.Dịch nghĩa\", \"c_Dịch nghĩa\", \n",
    "\"c Phien nghia:\", \"c. Phien nghia:\", \"c.Phien nghia:\", \"c_Phien nghia:\", \"c Phien nghia\", \"c. Phien nghia\", \"c.Phien nghia\", \"c_Phien nghia\", \n",
    "\"c Dich nghia:\", \"c. Dich nghia:\", \"c.Dich nghia:\", \"c_Dich nghia:\", \"c Dich nghia\", \"c. Dich nghia\", \"c.Dich nghia\", \"c_Dich nghia\", \"C Dich ngbia:\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_strings(strings, min_length=4):\n",
    "    return [s for s in strings if len(s) >= min_length]\n",
    "def is_header(text):\n",
    "    return is_similar(text,\"Phiên âm\")\n",
    "def contains_header_char(text):\n",
    "    return any((is_header(text) or Is_y1(char) or Is_y2(char)) for char in text)\n",
    "def remove_header_strings(strings):\n",
    "    return [s for s in strings if not contains_header_char(s)]\n",
    "def is_similar(word1, word2, threshold=5):\n",
    "    if word1 == \"Than ôi !\" or word1 ==\"Than ôi!\" or word1 ==\"Than ôi\" or word1 ==\"Than ôi \" or word1 ==\"Tháng\" or word1 ==\"tháng\" or word1 == \"2 in\" or word1 == \"2 năm\":\n",
    "        return False\n",
    "    elif word1 == \"b. Phiêpâm:\" or word1 == \"b. Phiên âm: tông,\" or word1 ==\"b. Phlên Am:\" or word1 == \"b. PAMAAm:\":\n",
    "        return True\n",
    "    distance = Levenshtein.distance(word1, word2)\n",
    "    return distance <= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_vietnamese(text):\n",
    "    vietnamese_pattern = r'^[0-9a-zA-ZàáảãạâầấẩẫậăằắẳẵặđèéẻẽẹêềếểễệìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶĐÈÉẺẼẸÊỀẾỂỄỆÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴ]+$'\n",
    "    return bool(re.fullmatch(vietnamese_pattern, text))\n",
    "def contains_vietnamese_char(text):\n",
    "    return any(is_vietnamese(char) for char in text)\n",
    "def remove_vietnamese_strings(strings):\n",
    "    return [s for s in strings if not contains_vietnamese_char(s['text'])]\n",
    "def remove_punctuation(input_string):\n",
    "    return re.sub(r\"[:'\\\".,!?;-_]\", \"\", input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DataFrame(Viet_str, Han_str):\n",
    "    df = pd.DataFrame(Viet_str)\n",
    "   \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_OCR(input_folder_path):\n",
    "    result = reader.readtext(input_folder_path)\n",
    "    filtered_result = [\n",
    "    {\n",
    "        \"Text\": re.sub(r'_', ' ', item[1]),\n",
    "        \"Image box\": [[int(point[0]), int(point[1])] for point in item[0]],\n",
    "        \"Type\": \"y0\" if Is_y0(item[1]) else \"y1\" if Is_y1(item[1]) else \"y2\" if Is_y2(item[1]) else \"unknown\"\n",
    "    }\n",
    "    for item in result\n",
    "    ]\n",
    "    return filtered_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    Recursively normalizes data by converting:\n",
    "    - numpy types (e.g., np.int32, np.ndarray) to standard Python types.\n",
    "    - Floats to integers where applicable.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {key: normalize_data(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [normalize_data(item) for item in data]\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return [normalize_data(item) for item in data.tolist()]\n",
    "    elif isinstance(data, (np.int32, np.int64, np.float32, np.float64)):\n",
    "        return int(data)\n",
    "    elif isinstance(data, float) and data.is_integer():\n",
    "        return int(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def is_within_y_threshold(box1, box2, vertical_threshold=20):\n",
    "    \"\"\"Checks if the vertical distance between two boxes is within the given threshold.\"\"\"\n",
    "    y1_top = box1[0][1]\n",
    "    y2_top = box2[0][1]\n",
    "    return abs(y1_top - y2_top) < vertical_threshold\n",
    "\n",
    "\n",
    "def extract_top_left(image_box):\n",
    "    \"\"\"Extracts the top-left corner coordinates (x, y) from an image box.\"\"\"\n",
    "    return image_box[0][0], image_box[0][1]\n",
    "\n",
    "\n",
    "def sort_and_merge(data, vertical_threshold=20):\n",
    "    \"\"\"\n",
    "    Sorts and merges text boxes based on proximity.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The input data containing Image box, Text, and Type.\n",
    "    - vertical_threshold: The maximum vertical distance to group text boxes.\n",
    "    \"\"\"\n",
    "    normalized_data = normalize_data(data)\n",
    "\n",
    "    df = pd.DataFrame(normalized_data, columns=['Image box', 'Text', 'Type'])\n",
    "    \n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    coords_df = df['Image box'].apply(extract_top_left).apply(pd.Series)\n",
    "    coords_df.columns = ['x1', 'y1']\n",
    "    df = pd.concat([df, coords_df], axis=1)\n",
    "    \n",
    "    df = df.sort_values(by='y1', ascending=True)\n",
    "    df['group'] = (df['y1'].diff().abs() > vertical_threshold).cumsum()\n",
    "    df = df.sort_values(by=['group', 'x1'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    merged_strings = []\n",
    "    visited = [False] * len(df)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if not visited[i]:\n",
    "            current_group = [df.iloc[i]['Text']]\n",
    "\n",
    "            if re.search(r'\\d', df.iloc[i]['Text']):\n",
    "                merged_strings.append(df.iloc[i]['Text'])\n",
    "                visited[i] = True\n",
    "                continue\n",
    "\n",
    "            visited[i] = True\n",
    "\n",
    "            for j in range(i + 1, len(df)):\n",
    "                if not visited[j]:\n",
    "                    if is_within_y_threshold(df.iloc[i]['Image box'], df.iloc[j]['Image box'], vertical_threshold):\n",
    "                        if re.search(r'\\d', df.iloc[j]['Text']):\n",
    "                            merged_strings.append(df.iloc[j]['Text'])\n",
    "                        else:\n",
    "                            current_group.append(df.iloc[j]['Text'])\n",
    "                        visited[j] = True\n",
    "\n",
    "            if current_group:\n",
    "                merged_text = \" \".join(current_group)\n",
    "                merged_strings.append(merged_text)\n",
    "\n",
    "    return merged_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_Han(input_folder_path):\n",
    "    \"\"\"\n",
    "    Hàm xử lý các file JSON trong thư mục đầu vào để trích xuất dữ liệu Hán và Việt.\n",
    "\n",
    "    Parameters:\n",
    "    - input_folder_path: Đường dẫn tới thư mục chứa file JSON.\n",
    "    - img_folder: Thư mục chứa hình ảnh OCR (dùng để xử lý đường dẫn).\n",
    "\n",
    "    Returns:\n",
    "    - Một DataFrame chứa dữ liệu đã xử lý.\n",
    "    \"\"\"\n",
    "    data = [] \n",
    "\n",
    "    for file_name in os.listdir(input_folder_path):\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "        if os.path.isfile(file_path) and file_name.endswith('.json'):\n",
    "            # Đọc và xử lý dữ liệu từ JSON\n",
    "            json_data = read_json_file(file_path)\n",
    "            extracted_data = extract_text_and_position(json_data)\n",
    "            han_text = remove_vietnamese_strings(extracted_data)\n",
    "\n",
    "            # Xử lý thông tin trang\n",
    "            page = file_name.replace(\"cropped_page\", \"\").replace(\".json\", \"\").replace(\"_\", \"\")\n",
    "            try:\n",
    "                page_num = int(page)\n",
    "            except ValueError:\n",
    "                print(f\"Không thể chuyển đổi {page} thành số nguyên.\")\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(img_folder, f\"full_page_{page_num}.png\")\n",
    "            img_path1 = os.path.join(img_folder, f\"full_page_{page_num + 1}.png\")\n",
    "            print(img_path)\n",
    "\n",
    "            viet_text = []\n",
    "            if os.path.exists(img_path):\n",
    "                viet_text.extend(remove_short_strings(sort_and_merge(process_OCR(img_path))))\n",
    "            if os.path.exists(img_path1):\n",
    "                viet_text.extend(remove_short_strings(sort_and_merge(process_OCR(img_path1))))\n",
    "\n",
    "            viet_text1 = []\n",
    "            header = False\n",
    "            for s in viet_text:\n",
    "                if not header:\n",
    "                    if is_header(s):\n",
    "                        header = True\n",
    "                else:\n",
    "                    viet_text1.append(s)\n",
    "            viet_text1 = \" \".join(viet_text1)\n",
    "            viet_text1 = viet_text1.lower()\n",
    "            viet_text1 = remove_punctuation(viet_text1)\n",
    "            viet_text1 = viet_text1.split(\" \")\n",
    "            viet_text1 = [s for s in viet_text1 if s != \"\" and s != \" \"]\n",
    "            viet_text1 = viet_text1[:sum(len(han['text']) for han in han_text)]  \n",
    "            viet_text_idx = 0\n",
    "            idx = 0\n",
    "            for han in han_text:\n",
    "                han_chars = list(han['text'])\n",
    "                viet_words = []\n",
    "                for i in range(len(han_chars)):\n",
    "                    if viet_text_idx < len(viet_text1):\n",
    "                        viet_words.append(viet_text1[viet_text_idx])\n",
    "                        viet_text_idx += 1\n",
    "                    else:\n",
    "                        viet_words.append(\"\")\n",
    "\n",
    "                viet_text = \" \".join(viet_words)\n",
    "                idx += 1\n",
    "                data.append({\n",
    "                    \"Image_name\": \"Sac_phong_trieu_Nguyen_tren_dia_ban_Thua_Thien_Hue_page\" + str(page).zfill(3) + \".png\",\n",
    "                    \"ID\": \"Sac_phong_trieu_Nguyen_tren_dia_ban_Thua_Thien_Hue.\" + str(page).zfill(3) + \".\" + str(idx).zfill(3),\n",
    "                    \"Image Box\": han['position'],\n",
    "                    \"SinoNom OCR\": han['text'],\n",
    "                    \"Chữ quốc ngữ\": viet_text\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './Json_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mextract_Han\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36mextract_Han\u001b[1;34m(input_folder_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mHàm xử lý các file JSON trong thư mục đầu vào để trích xuất dữ liệu Hán và Việt.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m- Một DataFrame chứa dữ liệu đã xử lý.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     15\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder_path, file_name)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path) \u001b[38;5;129;01mand\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;66;03m# Đọc và xử lý dữ liệu từ JSON\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './Json_result'"
     ]
    }
   ],
   "source": [
    "\n",
    "output = extract_Han(json_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
